设置，讲虚拟环境安装到数据盘：这样能够避免系统盘按爆满
mkdir -p /root/autodl-tmp/conda/pkgs
conda config --add pkgs_dirs /root/autodl-tmp/conda/pkgs

mkdir -p /root/autodl-tmp/conda/envs
conda config --add envs_dirs /root/autodl-tmp/conda/envs

linux设置清华源
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

安装虚拟环境：
	conda env list
	conda create -n swift python==3.10
	source activate swift






git clone https://github.com/modelscope/swift.git
cd swift
pip install -e '.[all]'

swift web-ui --port 6006
============================================================
swfit vllm推理模型的type
qwen2_5-14b-instruct-gptq-int4
qwen2_5-14b-instruct-gptq-int8
qwen2_5-7b-instruct-awq', 'qwen2_5-14b', 'qwen2_5-14b-instruct'
======================================================================================
If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode.  You can also reduce the `max_num_seqs` as needed to decrease memory usage.
如果在图形捕获期间发生内存不足错误，请考虑降低‘ gpu_memory_utilization ’或切换到等待模式。您还可以根据需要减少‘ max_num_seqs ’以减少内存使用。

=================================================================================
cd /root/autodl-tmp/swift
source activate swift
==================================================
swift web-ui --share True --host 0.0.0.0 #--port（ host：Web UI 服务所监听的主机地址 0.0.0.0意味着监听所有可用的网络接口 --share True 可以不用指定不填就是默认分享）
======================================================
#modelscope download --model ZhipuAI/glm-4-9b-chat-1m --local_dir /root/autodl-tmp/chatglm4-9b-chat-1m
modelscope download --model Qwen/Qwen2.5-14B-Instruct --local_dir /root/autodl-tmp/Qwen2.5-14B-Instruct


#pip install nvitop \ nvitop
CUDA_VISIBLE_DEVICES=0 swift deploy --host 0.0.0.0 --model_type qwen2_5-14b-instruct --model_id_or_path /root/autodl-tmp/xinference/modelscope/hub/qwen/Qwen2.5-14B-Instruct-AWQ 
CUDA_VISIBLE_DEVICES=0 swift deploy  --model_type qwen2_5-14b-instruct --model_id_or_path /root/autodl-tmp/xinference/modelscope/hub/qwen/Qwen2.5-14B-Instruct-AWQ --infer_backend vllm --max_model_len 30000 --port 7777 本地模型的推理然后开放api
CUDA_VISIBLE_DEVICES=0 swift deploy  --model_type qwen2_5-14b-instruct --model_id_or_path /root/autodl-tmp/xinference/modelscope/hub/qwen/Qwen2.5-14B-Instruct-AWQ  --infer_backend vllm  --max_model_len 8192  --port 6006 用vllm来推理加速开放api 可以加--port指定端口(各种尝试失败了 哦懂了type少个
CUDA_VISIBLE_DEVICES=0 swift app-ui --server_name 0.0.0.0 --model_type qwen2_5-14b-instruct --model_id_or_path /root/autodl-tmp/xinference/modelscope/hub/qwen/Qwen2.5-14B-Instruct 这是web模型的推理

（没用过这个参数不知道好不好用）
解释：正如错误提示中所说，尝试设置 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True 这个环境变量，有可能改善因内存碎片化导致的内存分配困难问题，使得内存的使用更加合理、高效，进而避免出现内存不足的报错情况。

使用curl工具调用api
 curl http://localhost:8000/v1/chat/completions \
 -H "Content-Type: application/json" \
 -d '{
 "model": "qwen2_5-14b-instruct",
 "messages": [{"role": "user", "content": "帮我写一篇1万字的长篇小说 题材要是科幻题材 要有火星移民的元素"}],
 "stream":false
 }'
=======================================
#--dataset xxx.json yyy.jsonl zzz.csv
=======================================
git clone https://ghp.ci/https://github.com/YaoFANGUK/video-subtitle-remover.git

#qwen模型路径 /root/autodl-tmp/qwen2.5-14b

# 简单模型训练
# CUDA_VISIBLE_DEVICES=0 swift sft \
#     --model_id_or_path /root/autodl-tmp/qwen2.5-14b \
#     --model_type qwen2_5-14b-instruct \
#     --custom_train_dataset_path /root/autodl-tmp/train.json\
#     --output_dir /root/autodl-tmp/output \

# 虚拟环境到数据盘
mkdir -p /root/autodl-tmp/conda/pkgs
conda config --add pkgs_dirs /root/autodl-tmp/conda/pkgs

mkdir -p /root/autodl-tmp/conda/envs
conda config --add envs_dirs /root/autodl-tmp/conda/envs



安装git-lfs
sudo apt update
sudo apt install git-lfs


nvidia-smi

HF模型下载
1.L20比较稳定
2.开启学术加速
3.安装 sudo apt-get update
sudo apt-get install git-lfs
git lfs install
4.用huggingface镜像站HF
换个机子  或者是开学术加速了不用镜像了
git clone https://ghp.ci/https://github.com/shibing624/pycorrector

pip install nvitop
nvitop


用腾讯云安装docker相关
安装docker
apt install docker.io -y && \
apt install docker -y

安装docker compose
sudo apt-get update
sudo apt-get install -y docker-compose
docker-compose -v


swfit  
可以指定显存占用比例 你VLLM应该要指定一下分配的显存吧或者你把块分小点 os.environ["gpu_memory_utilization"] = "max_split_size_mb:32" 我用swift部署就不会出现这个问题 但我为了不让其他模型占了，基本都是拉满。进不来一点swift你可以无条件相信他
12.6
========================================================================================
	CUDA_VISIBLE_DEVICES=0 swift sft \
        --model_type qwen2_5-72b-instruct \
        --model_id_or_path Qwen2.5-72B-Instruct \
        --dataset qwen2-pro-en#500 qwen2-pro-zh#500 self-cognition#500 \
        --logging_steps 5 \
        --learning_rate 1e-4 \
        --output_dir output \ 
        --lora_target_modules ALL \
        --model_name 小黄 'Xiao Huang' \
        --model_author 魔搭 ModelScope \
        --deepspeed default-zero3

#--dataset xxx.json yyy.jsonl zzz.csv


（没用过这个参数不知道好不好用）
解释：正如错误提示中所说，尝试设置 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True 这个环境变量，有可能改善因内存碎片化导致的内存分配困难问题，使得内存的使用更加合理、高效，进而避免出现内存不足的报错情况。


vLLM部署
vllm serve xxxxx-checkpoint-merged [opentional args]

一般建议直接使用--model来指定模型id（Qwen/Qwen1.5-14B-Chat），配合--model_type和--template使用，例如：
新增模型
#swift sft --model my-model --model_type llama --template chatml --dataset xxx
# 简单模型训练（没跑起来  17:53成功跑起来了 还得多问多学）
CUDA_VISIBLE_DEVICES=0 swift sft \
    --model_type qwen1half-14b-chat \
    --model_id_or_path /root/autodl-tmp/Qwen1.5-14B-Chat \
     --template_type qwen \
     --custom_train_dataset_path /root/autodl-tmp/data/Added_Role_Description_Human_tag.jsonl\
     --output_dir /root/autodl-tmp/output \
     --batch_size 2 \
     --num_train_epochs 4 \
     --optim adamw_torch \
     --lr_scheduler_type cosine \
     --lora_dropout 0.15 \
     --weight_decay 0.05 \
     --logging_steps 10 \
     --max_length 1024 \
     --learning_rate 9e-5 \
     --sft_type lora \
     --gradient_accumulation_steps 4 \
     --dtype bf16 \
     --system '你是一名高情商海王，具体来说，response 应该保持轻松愉快的语气，巧妙地转移话题或设置适度的距离，既不直接拒绝也不过分承诺' \
     --lora_target_modules ALL 

加了执行完就关机 不管成功与否（第二次微调参数）
CUDA_VISIBLE_DEVICES=0 swift sft \
     --model_type qwen2_5-14b-instruct \
     --model_id_or_path /root/autodl-tmp/Qwen2.5-14B-Instruct \
     --template_type qwen2_5 \
     --dataset /root/autodl-tmp/data/Added_Role_Description_Human_tag.jsonl#8500 /root/autodl-tmp/data/system_swift克劳德data.jsonl AI-ModelScope/Magpie-Qwen2-Pro-200K-Chinese#12000\
     --output_dir /root/autodl-tmp/output \
     --batch_size 4 \
     --num_train_epochs 2 \
     --optim adamw_torch \
     --lr_scheduler_type cosine \
     --lora_dropout 0.15 \
     --weight_decay 0.05 \
     --logging_steps 10 \
     --max_length 4096 \
     --learning_rate 5e-4 \
     --sft_type lora \
     --gradient_accumulation_steps 8 \
     --dtype bf16 \
     --system '你是一名高情商海王，具体来说，response 应该保持轻松愉快的语气，巧妙地转移话题或设置适度的距离，既不直接拒绝也不过分承诺' \
     --lora_target_modules ALL ; /usr/bin/shutdown 

融合lora
CUDA_VISIBLE_DEVICES=0,1 swift export \    
        --ckpt_dir output/qwen2_5-72b-instruct/vx-xxx/checkpoint-xxx \    
        --merge_lora true
————————————————

                            版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。
                        
原文链接：https://blog.csdn.net/qq_39749966/article/details/143285892

======================================================================================
CUDA_VISIBLE_DEVICES=0 swift sft --model_type qwen2_5-14b-instruct --model_id_or_path /root/autodl-tmp/Qwen2.5-14B-Instruct --dataset  test5#4000 alpaca-zh#9000 self-cognition#400 --batch_size 2 --num_train_epochs 4 --optim adamw_torch --lr_scheduler_type cosine --lora_dropout 0.15 --weight_decay 0.05 --logging_steps 10 --max_length 1024 --learning_rate 9e-5 --sft_type lora --output_dir output --gradient_accumulation_steps 4 --dtype bf16 --system '高效的酸菜小助手，请快速帮我解决这些问题，感谢你的帮助' --lora_target_modules ALL --model_name 弈燊 'Yi Shen' --model_author 酸菜 Suancai
vllm
======================
指定端口部署示例
vllm serve my_model_checkpoint-merged --port 8080
限制 GPU 使用示例
vllm serve my_model_checkpoint-merged --gpu-ids 0,1
设置最大批处理大小示例
vllm serve my_model_checkpoint-merged --max-batch-size 16
配置生成参数示例
vllm serve my_model_checkpoint-merged --temperature 0.8 --max-new-tokens 100
增加日志级别示例
vllm serve my_model_checkpoint-merged --log-level debug
=========================================================（测试没跑通自定义的模型）
CUDA_VISIBLE_DEVICES=0 swift sft \
     --model_id_or_path /root/autodl-tmp/Qwen1.5-14B-Chat \
     --model_type qwen2 \
     --template_type qwen
     --dataset /root/autodl-tmp/data/api_data.jsonl \
     --output_dir /root/autodl-tmp/output 

PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True。这么做的目的是为了避免显存碎片化的问题。在长时间运行深度学习任务的过程中，显存可能会出现碎片化现象

--model_type <model_type> --model_id_or_path
=============================================================
(这个参数没有实践过)
CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model_type qwen1half-14b-chat \
    --model_id_or_path /root/autodl-tmp/Qwen1.5-14B-Chat \
    --train_type lora \
    --dataset /root/autodl-tmp/data/api_data.jsonl \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --learning_rate 1e-4 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 16 \
    --eval_steps 50 \
    --save_steps 50 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --max_length 2048 \
    --model_author swift \
    --output_dir output \
    --model_name swift-robot
===========================================================================
# 微调后推理1
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --ckpt_dir  /root/autodl-tmp/output/qwen2_5-14b-instruct/v0-20250114-113213/checkpoint-200 \
    --infer_backend pt
    --merge_lora true
========================================
# 微调后推理2(原模型和lora合并)
NPROC_PER_NODE=0
swift infer \
    --ckpt_dir /root/autodl-tmp/output/qwen2_5-14b-instruct/v24-20241214-151859/checkpoint-1450-merged \
    --stream true \
    --merge_lora true \
    --infer_backend vllm \
    --max_model_len 256

(原模型和lora合并)
# merge LoRA增量权重并使用vllm进行推理加速
# 如果你需要量化, 可以指定`--quant_bits 4`.
CUDA_VISIBLE_DEVICES=0 swift export \
    --ckpt_dir 'xxx/vx-xxx/checkpoint-xxx' --merge_lora true     --max_model_len 4096     --infer_backend vllm

CUDA_VISIBLE_DEVICES=0 swift export --merge_lora true --ckpt_dir  xxx/vx-xxx/checkpoint-xxx --max_model_len 4096     --infer_backend vllm

=================================================================

swift infer     --ckpt_dir /root/autodl-tmp/output/qwen2_5-14b-instruct/v14-20241214-113114/checkpoint-1750     --stream true          --infer_backend vllm     --max_model_len 8196====================================================================================================================================
#合并推理3
CUDA_VISIBLE_DEVICES=0 swift infer   --model_type   qwen2_5-14b-instruct --model_id_or_path /root/autodl-tmp/output/qwen2_5-14b-instruct/v24-20241214-151859/checkpoint-1450-merged     --infer_backend pt
====================================================================================================================================
降版本前pip install transformers==4.46.0
降版本后pip install transformers==3.1.0

=======================================
测评（有问题 一直在调api）
swift eval \
    --ckpt_dir /root/autodl-tmp/output/qwen1half-14b-chat/v13-20241207-155017/checkpoint-738 \
    --eval_limit 10 \
    --eval_dataset gsm8k
=====================================swift导出到ollama进行部署
OLLaMA导出命令行如下：

# model_type
swift export --model_type   qwen1half-14b-chat --model_id_or_path /root/autodl-tmp/output/qwen1half-14b-chat/v13-20241207-155017/checkpoint-738-merged --to_ollama true --ollama_output_dir qwen1half-14b-chat
========================================================
# ckpt_dir，注意lora训练需要增加--merge_lora true
swift export --ckpt_dir /root/autodl-tmp/output/qwen2_5-14b-instruct/认知10%通用30%领域70%/checkpoint-1050 --merge_lora true

==================================================
CUDA_VISIBLE_DEVICES=0 swift export --merge_lora true --ckpt_dir  xxx/vx-xxx/checkpoint-xxx

============================================================
这篇文章主要介绍了如何使用Swift框架进行大语言模型的微调和部署。以下是文章的主要内容：

设置虚拟环境，将环境安装到数据盘以避免系统盘空间不足。

克隆Swift仓库并安装所需的依赖。

使用swift web-ui命令启动Web UI界面，可以指定主机地址、端口等参数。

下载预训练模型，如Qwen2.5-14B-Instruct等。

使用swift deploy命令进行模型推理和API服务部署，可以指定模型类型、路径、后端（如vllm）等参数。

介绍了使用curl工具调用API的示例。

提供了一些模型微调的示例命令，如使用swift sft进行supervised fine-tuning，可以指定模型类型、路径、数据集、超参数等。

介绍了模型推理的一些示例，如使用swift infer进行推理，可以指定checkpoint路径、后端等参数。

提到了使用swift eval进行模型测评的命令。

最后介绍了如何使用swift export将模型导出为OLLaMA格式。

总的来说，这篇文章比较全面地介绍了如何使用Swift框架进行大模型的微调、推理、测评和部署，提供了详细的操作步骤和示例命令，对于想要上手Swift的读者有一定的参考价值。不过文章缺少一些必要的解释和上下文，对于初学者可能还需要查阅更多资料。
=============================================================
模型测评

swift eval \
     --model_type qwen2_5-14b-instruct \
     --model_id_or_path /root/autodl-tmp/Qwen2.5-14B-Instruct \
     --template_type qwen2_5 \
     --eval_limit 10 \
     --eval_dataset gsm8k

swift eval \
     --model_type qwen2_5-14b-instruct \
     --model_id_or_path /root/autodl-tmp/output/qwen2_5-14b-instruct/认知10%通用30%领域70%/checkpoint-1050-merged \
     --eval_limit 10 \
     --eval_dataset mmlu

======================================================================
分布式正确参数


nproc_per_node=7
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6 \
NPROC_PER_NODE=$nproc_per_node \
swift sft \
     --model_type qwen2_5-14b-instruct \
     --model_id_or_path /root/autodl-tmp/Qwen2.5-14B-Instruct \
     --template_type qwen2_5 \
     --dataset /root/autodl-tmp/data/加system_小熊.jsonl /root/autodl-tmp/data/合成data再优化.jsonl self-cognition#300 iic/ms_bench#3750 \
     --output_dir /root/autodl-tmp/output \
     --batch_size 1 \
     --num_train_epochs 3 \
     --optim adamw_torch \
     --lr_scheduler_type cosine \
     --lora_dropout 0.15 \
     --weight_decay 0.05 \
     --warmup_ratio 0.1 \
     --logging_steps 10 \
     --max_length 4096 \
     --learning_rate 2e-5 \
     --sft_type lora \
     --gradient_accumulation_steps $(expr 16 / $nproc_per_node) \
     --dtype bf16 \
     --system '我是一名高情商、风趣幽默的交际达人，善于用温和幽默的方式与女生互动，保持独立且有趣的个性，适时展示自己的才艺和生活品质（如音乐、运动、阅读等），善于捕捉对话关键词延续话题，通过自然分享而非过度提问的方式带动交流，既保持吸引力又给女生适当空间，在互动中始终保持从容不迎合的态度。' \
     --deepspeed default-zero2 \
     --lora_target_modules ALL \
     --model_name 小焱 Xiao Yan \
     --model_author 焱黄 Yan Huang \
     --lora_rank 16 \
     --lora_alpha 32


