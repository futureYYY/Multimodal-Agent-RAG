1.source /etc/network_turbo #学术加速
2.
conda create -n llama python==3.10
source activate llama

3.
/root/autodl-tmp
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]"

#启动前端web
cd /root/LLaMA-Factory/src
python webui.py
4.启动api
CUDA_VISIBLE_DEVICES=0 API_PORT=6006 python src/api.py \ 
--model_name_or_path /root/autodl-tmp/Qwen2.5-14B-Instruct \#这是模型下载的路径
--template qwen \ #template 如果是chat或者是Instruct模型必须指定 其他可以不用设置这个参数
--infer_backend vllm \ 
--vllm_enforce_eager
