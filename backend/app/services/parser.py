"""
æ–‡ä»¶è§£ææœåŠ¡
æ”¯æŒ PDF, Word, Excel, CSV, TXT æ ¼å¼
"""

import os
import fitz  # PyMuPDF
from typing import List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import pandas as pd
from docx import Document as DocxDocument
from docx.table import Table
from docx.text.paragraph import Paragraph

from app.core.config import get_settings

settings = get_settings()


@dataclass
class ParsedChunk:
    """è§£æåçš„å—"""
    content: str
    page_number: Optional[int]
    content_type: str  # text, table, image
    image_path: Optional[str] = None
    metadata: Optional[dict] = None


class TextSplitter:
    """æ–‡æœ¬åˆ‡åˆ†å™¨"""

    def __init__(
        self,
        chunk_size: int = None,
        chunk_overlap: int = None,
        separator: str = "\n\n",
    ):
        self.chunk_size = chunk_size or settings.CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or settings.CHUNK_OVERLAP
        self.separator = separator.replace("\\n", "\n") if separator else "\n\n"

    def split(self, text: str) -> List[str]:
        """åˆ‡åˆ†æ–‡æœ¬"""
        if not text:
            return []
            
        if len(text) <= self.chunk_size:
            return [text]

        chunks = []
        start = 0

        while start < len(text):
            # å¦‚æœå‰©ä½™é•¿åº¦å°äºå—å¤§å°ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªå—
            if len(text) - start <= self.chunk_size:
                chunks.append(text[start:].strip())
                break

            end = start + self.chunk_size

            # å°è¯•åœ¨è‡ªç„¶è¾¹ç•Œå¤„åˆ‡åˆ†
            best_end = end
            
            # æŸ¥æ‰¾æœ€è¿‘çš„åˆ†éš”ç¬¦ï¼ˆå‘å‰æŸ¥æ‰¾ï¼‰
            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„åˆ†éš”ç¬¦ï¼Œå…¶æ¬¡æ˜¯é€šç”¨åˆ†éš”ç¬¦
            separators = [self.separator, "\n", "ã€‚", ".", " "]
            
            found_sep = False
            for sep in separators:
                # åœ¨ [start + chunk_size/2, end] èŒƒå›´å†…æŸ¥æ‰¾
                min_search_pos = max(start + self.chunk_size // 2, start)
                sep_pos = text.rfind(sep, min_search_pos, end)
                if sep_pos > start:
                    best_end = sep_pos + len(sep)
                    found_sep = True
                    break
            
            if not found_sep:
                # å¼ºåˆ¶åˆ‡åˆ†
                best_end = end

            chunk = text[start:best_end].strip()
            if chunk:
                chunks.append(chunk)
            
            # ä¸‹ä¸€å—çš„èµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
            start = best_end - self.chunk_overlap
            
            # é˜²æ­¢æ­»å¾ªç¯
            if start <= start: # å¦‚æœé‡å å¯¼è‡´æ²¡å‰è¿›
                 start = best_end

        return chunks


class FileParser:
    """æ–‡ä»¶è§£æå™¨"""

    def __init__(self, kb_id: str):
        self.kb_id = kb_id
        self.image_dir = os.path.join(settings.IMAGE_DIR, kb_id)
        os.makedirs(self.image_dir, exist_ok=True)

    def parse(
        self, 
        file_path: str, 
        chunk_mode: str = "auto", # auto, no_chunk, custom
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        separator: str = "\n\n"
    ) -> List[ParsedChunk]:
        """
        è§£ææ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            chunk_mode: åˆ‡åˆ†æ¨¡å¼
            chunk_size: å—å¤§å°
            chunk_overlap: é‡å å¤§å°
            separator: åˆ†éš”ç¬¦
        """
        self.splitter = TextSplitter(chunk_size, chunk_overlap, separator)
        self.chunk_mode = chunk_mode
        
        ext = os.path.splitext(file_path)[1].lower()

        if ext == ".pdf":
            return self._parse_pdf(file_path)
        elif ext == ".docx":
            return self._parse_docx(file_path)
        elif ext == ".xlsx":
            return self._parse_excel(file_path)
        elif ext == ".csv":
            return self._parse_csv(file_path)
        elif ext == ".txt":
            return self._parse_txt(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")

    def _save_image(self, image_bytes: bytes, file_id: str, page_num: int, img_index: int, ext: str = "png") -> str:
        """ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°"""
        image_filename = f"{file_id}_p{page_num}_i{img_index}.{ext}"
        image_path = os.path.join(self.image_dir, image_filename)
        
        with open(image_path, "wb") as f:
            f.write(image_bytes)
            
        # è¿”å›ç›¸å¯¹è·¯å¾„æˆ–æ–‡ä»¶åï¼Œç”¨äºå‰ç«¯å±•ç¤ºå’Œåç»­å¤„ç†
        # è¿™é‡Œè¿”å›æ–‡ä»¶åï¼Œå‰ç«¯é€šè¿‡ /static/images/{kb_id}/{filename} è®¿é—®
        # æˆ–è€…åç«¯ç»Ÿä¸€å¤„ç†è·¯å¾„
        return f"{self.kb_id}/{image_filename}"

    def _process_text_content(self, text: str, page_num: int) -> List[ParsedChunk]:
        """å¤„ç†æ–‡æœ¬å†…å®¹ï¼ˆæ ¹æ®åˆ‡åˆ†ç­–ç•¥ï¼‰"""
        if not text.strip():
            return []
            
        chunks = []
        # å¤„ç†ä¸åˆ‡åˆ†æ¨¡å¼
        if self.chunk_mode == "no_split" or self.chunk_mode == "no_chunk":
            # å³ä½¿ä¸åˆ‡åˆ†ï¼Œä¹Ÿå¯èƒ½éœ€è¦å¤„ç†ä¸€äº›åŸºæœ¬çš„æ¸…ç†
            cleaned_text = text.strip()
            if cleaned_text:
                chunks.append(ParsedChunk(
                    content=cleaned_text,
                    page_number=page_num,
                    content_type="text"
                ))
        else:
            text_chunks = self.splitter.split(text)
            for tc in text_chunks:
                chunks.append(ParsedChunk(
                    content=tc,
                    page_number=page_num,
                    content_type="text"
                ))
        return chunks

    def _parse_pdf(self, file_path: str) -> List[ParsedChunk]:
        """è§£æ PDF æ–‡ä»¶ (åŒ…å«å›¾ç‰‡æå–å’Œé¡ºåºä¿æŒ)"""
        chunks = []
        doc = fitz.open(file_path)
        file_id = os.path.basename(file_path).split("_")[0]

        for page_num, page in enumerate(doc, start=1):
            # è·å–é¡µé¢ä¸Šçš„æ‰€æœ‰å— (text, image)
            # sort=True ä¼šæ ¹æ®å‚ç›´åæ ‡æ’åºï¼Œç¬¦åˆé˜…è¯»é¡ºåº
            blocks = page.get_text("dict", sort=True)["blocks"]
            
            # å¦‚æœæ˜¯ä¸åˆ‡åˆ†æ¨¡å¼ï¼Œæˆ‘ä»¬åœ¨é¡µé¢çº§åˆ«èšåˆæ‰€æœ‰æ–‡æœ¬
            # å¦åˆ™æˆ‘ä»¬ä¾ç„¶åœ¨ block çº§åˆ«å¤„ç†ï¼Œä»¥ä¾¿æ­£ç¡®æ’å…¥å›¾ç‰‡
            page_text_buffer = ""
            
            # ä¸´æ—¶å­˜å‚¨æœ¬é¡µçš„chunksï¼Œæœ€åå†æ ¹æ®æ¨¡å¼å†³å®šå¦‚ä½•åˆå¹¶
            page_chunks = []
            current_text_buffer = ""

            for block_idx, block in enumerate(blocks):
                if block["type"] == 0:  # Text Block
                    # æå–æ–‡æœ¬
                    block_text = ""
                    for line in block["lines"]:
                        for span in line["spans"]:
                            block_text += span["text"]
                        block_text += "\n"
                    
                    if self.chunk_mode == "no_split" or self.chunk_mode == "no_chunk":
                        page_text_buffer += block_text
                    else:
                        current_text_buffer += block_text
                    
                elif block["type"] == 1:  # Image Block
                    # 2. å¤„ç†å›¾ç‰‡
                    try:
                        image_bytes = block["image"]
                        image_ext = block["ext"]
                        img_size = len(image_bytes)
                        
                        # è·å–å›¾ç‰‡å°ºå¯¸ä¿¡æ¯ (ç”¨äºè°ƒè¯•å’Œè¿‡æ»¤)
                        width, height = 0, 0
                        try:
                            # å°è¯•ä» bytes åˆ›å»º pixmap è·å–å°ºå¯¸
                            pix = fitz.Pixmap(image_bytes)
                            width, height = pix.width, pix.height
                            pix = None # é‡Šæ”¾èµ„æº
                        except Exception:
                            pass

                        # æ‰“å°è°ƒè¯•ä¿¡æ¯
                        print(f"ğŸ” [PDF Image Debug] Page: {page_num} | Size: {img_size} bytes ({img_size/1024:.2f} KB) | Dim: {width}x{height} | Ext: {image_ext}")

                        # è¿‡æ»¤è¿‡å°çš„å›¾ç‰‡ (å°äº 3KB)
                        if img_size < 3072:
                            print(f"   -> SKIPPED (Size < 3KB)")
                            # å¦‚æœå›¾ç‰‡æ— æ•ˆï¼Œä¸æ‰“æ–­æ–‡æœ¬æµ
                            continue
                        
                        # è¿‡æ»¤æç«¯é•¿å®½æ¯”çš„å›¾ç‰‡ (é€šå¸¸æ˜¯åˆ†å‰²çº¿)
                        # ä¾‹å¦‚: å®½åº¦æ˜¯é«˜åº¦çš„ 50 å€ï¼Œæˆ–è€…é«˜åº¦æ˜¯å®½åº¦çš„ 50 å€
                        if width > 0 and height > 0:
                            ratio = width / height
                            if ratio > 50 or ratio < 0.02:
                                print(f"   -> SKIPPED (Extreme Aspect Ratio: {ratio:.2f})")
                                continue

                        # åªæœ‰å›¾ç‰‡æœ‰æ•ˆæ—¶ï¼Œæ‰ç»“ç®—ä¹‹å‰çš„æ–‡æœ¬ (ä»…åœ¨é no_split æ¨¡å¼ä¸‹)
                        if self.chunk_mode != "no_split" and self.chunk_mode != "no_chunk":
                            if current_text_buffer:
                                page_chunks.extend(self._process_text_content(current_text_buffer, page_num))
                                current_text_buffer = ""

                        # ä¿å­˜å›¾ç‰‡
                        saved_path = self._save_image(image_bytes, file_id, page_num, block_idx, image_ext)
                        
                        # åˆ›å»ºå›¾ç‰‡å— (å›¾ç‰‡æ€»æ˜¯å•ç‹¬æˆå—)
                        # æ³¨æ„ï¼šåœ¨ no_split æ¨¡å¼ä¸‹ï¼Œå›¾ç‰‡ä¹Ÿä¼šæˆä¸ºç‹¬ç«‹çš„å—æ’å…¥åœ¨æ–‡æœ¬ä¹‹é—´ï¼Œæˆ–è€…è¿½åŠ åœ¨æœ€åï¼Ÿ
                        # é€šå¸¸ no_split æ„å‘³ç€æ–‡æœ¬ä¸åˆ‡åˆ†ï¼Œä½†å›¾ç‰‡è¿˜æ˜¯ç‹¬ç«‹çš„
                        # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å…ˆæŠŠå›¾ç‰‡å­˜å…¥ page_chunks
                        page_chunks.append(ParsedChunk(
                            content=f"[å›¾ç‰‡: {saved_path}]", # å ä½ç¬¦å†…å®¹
                            page_number=page_num,
                            content_type="image",
                            image_path=saved_path,
                            metadata={
                                "timestamp": datetime.now().isoformat(),
                                "original_name": f"image_{page_num}_{block_idx}"
                            }
                        ))
                    except Exception as e:
                        print(f"PDF å›¾ç‰‡æå–å¤±è´¥ (Page {page_num}): {e}")
            
            # é¡µé¢ç»“æŸå¤„ç†
            if self.chunk_mode == "no_split" or self.chunk_mode == "no_chunk":
                # ä¸åˆ‡åˆ†æ¨¡å¼ï¼šå¤„ç†æ•´ä¸ªé¡µé¢çš„æ–‡æœ¬
                if page_text_buffer:
                    # å°†æ–‡æœ¬å—æ’å…¥åˆ° chunks å¼€å¤´ï¼ˆæˆ–è€…æ ¹æ®ä¸šåŠ¡é€»è¾‘ï¼‰
                    # è¿™é‡Œç®€å•çš„å¤„ç†ï¼šæ–‡æœ¬ä½œä¸ºä¸€ä¸ªå¤§å—ï¼Œå›¾ç‰‡ä½œä¸ºç‹¬ç«‹å—
                    # å¦‚æœå¸Œæœ›å›¾ç‰‡ç©¿æ’åœ¨æ–‡æœ¬ä¸­ï¼Œé‚£ no_split çš„å®šä¹‰å°±æ¯”è¾ƒæ¨¡ç³Šäº†
                    # æ—¢ç„¶æ˜¯ "æŒ‰é¡µä¸åˆ‡åˆ†"ï¼Œé‚£ä¹ˆè¿™ä¸€é¡µçš„æ‰€æœ‰æ–‡æœ¬åº”è¯¥æ˜¯ä¸€ä¸ªå—
                    chunks.extend(self._process_text_content(page_text_buffer, page_num))
                # è¿½åŠ æœ¬é¡µæå–çš„æ‰€æœ‰å›¾ç‰‡
                chunks.extend(page_chunks)
            else:
                # æ™®é€šåˆ‡åˆ†æ¨¡å¼ï¼šå¤„ç†å‰©ä½™çš„ buffer
                if current_text_buffer:
                    page_chunks.extend(self._process_text_content(current_text_buffer, page_num))
                chunks.extend(page_chunks)

        doc.close()
        return chunks

    def _parse_docx(self, file_path: str) -> List[ParsedChunk]:
        """è§£æ Word æ–‡ä»¶ (åŒ…å«å›¾ç‰‡æå–å’Œé¡ºåºä¿æŒ)"""
        chunks = []
        doc = DocxDocument(file_path)
        file_id = os.path.basename(file_path).split("_")[0]
        
        current_text_buffer = ""
        
        # è¾…åŠ©å‡½æ•°ï¼šå¤„ç†æ®µè½ä¸­çš„å›¾ç‰‡å’Œæ–‡æœ¬
        def process_paragraph(para, page_num):
            nonlocal current_text_buffer
            
            for run in para.runs:
                # 1. æå–æ–‡æœ¬
                if run.text:
                    current_text_buffer += run.text
                
                # 2. æ£€æŸ¥å›¾ç‰‡
                # æŸ¥æ‰¾ run å…ƒç´ ä¸‹çš„ drawing æ ‡ç­¾
                if 'drawing' in run.element.xml:
                    drawings = run.element.findall('.//{http://schemas.openxmlformats.org/wordprocessingml/2006/main}drawing')
                    for drawing in drawings:
                        # æ‰¾åˆ° blip å…ƒç´ è·å– rId
                        nsmap = {
                            'a': 'http://schemas.openxmlformats.org/drawingml/2006/main',
                            'r': 'http://schemas.openxmlformats.org/officeDocument/2006/relationships'
                        }
                        blips = drawing.findall('.//a:blip', namespaces=nsmap)
                        for blip in blips:
                            rId = blip.get('{http://schemas.openxmlformats.org/officeDocument/2006/relationships}embed')
                            if rId:
                                try:
                                    image_part = doc.part.related_parts[rId]
                                    image_bytes = image_part.blob
                                    
                                    # è¿‡æ»¤è¿‡å°çš„å›¾ç‰‡ (å°äº 2KB)
                                    if len(image_bytes) < 2048:
                                        print(f"ğŸ” [Parser] è·³è¿‡è¿‡å°å›¾ç‰‡ (Word): {len(image_bytes)} bytes")
                                        continue

                                    content_type = image_part.content_type
                                    ext = content_type.split('/')[-1] if '/' in content_type else "png"
                                    if ext == "jpeg": ext = "jpg"
                                    
                                    # ç»“ç®—å‰é¢çš„æ–‡æœ¬
                                    if current_text_buffer:
                                        chunks.extend(self._process_text_content(current_text_buffer, page_num))
                                        current_text_buffer = ""
                                    
                                    # ä¿å­˜å›¾ç‰‡
                                    img_idx = len(chunks) 
                                    saved_path = self._save_image(image_bytes, file_id, page_num, img_idx, ext)
                                    
                                    chunks.append(ParsedChunk(
                                        content=f"[å›¾ç‰‡: {saved_path}]",
                                        page_number=page_num,
                                        content_type="image",
                                        image_path=saved_path,
                                        metadata={
                                            "timestamp": datetime.now().isoformat(),
                                            "original_name": f"image_docx_{img_idx}"
                                        }
                                    ))
                                except Exception as e:
                                    print(f"Word å›¾ç‰‡æå–å¤±è´¥: {e}")
            
            # æ®µè½ç»“æŸæ¢è¡Œ
            current_text_buffer += "\n"

        # éå†æ–‡æ¡£å…ƒç´ 
        page_num_estimate = 1
        para_count = 0
        
        for element in doc.element.body.iterchildren():
            if element.tag.endswith('p'): # Paragraph
                process_paragraph(Paragraph(element, doc), page_num_estimate)
                para_count += 1
                if para_count > 10: # ç²—ç•¥ä¼°ç®—åˆ†é¡µ
                    para_count = 0
                    page_num_estimate += 1
                    
            elif element.tag.endswith('tbl'): # Table
                # ç»“ç®—æ–‡æœ¬
                if current_text_buffer:
                    chunks.extend(self._process_text_content(current_text_buffer, page_num_estimate))
                    current_text_buffer = ""
                
                # æå–è¡¨æ ¼å†…å®¹
                table = Table(element, doc)
                rows_data = []
                for row in table.rows:
                    row_cells = [cell.text.strip().replace("\n", " ") for cell in row.cells]
                    rows_data.append(row_cells)
                
                if rows_data:
                    df = pd.DataFrame(rows_data)
                    md_table = df.to_markdown(index=False, header=False)
                    chunks.append(ParsedChunk(
                        content=md_table,
                        page_number=page_num_estimate,
                        content_type="table",
                    ))

        # å¤„ç†å‰©ä½™æ–‡æœ¬
        if current_text_buffer:
            chunks.extend(self._process_text_content(current_text_buffer, page_num_estimate))
            
        return chunks

    def _parse_txt(self, file_path: str) -> List[ParsedChunk]:
        """è§£æ TXT æ–‡ä»¶"""
        try:
            with open(file_path, mode='r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            with open(file_path, mode='r', encoding='gbk') as f:
                content = f.read()
        
        return self._process_text_content(content, 1)

    def _parse_excel(self, file_path: str) -> List[ParsedChunk]:
        """è§£æ Excel æ–‡ä»¶"""
        chunks = []
        xlsx = pd.ExcelFile(file_path)

        for sheet_name in xlsx.sheet_names:
            df = pd.read_excel(xlsx, sheet_name=sheet_name)
            # Excel é€šå¸¸æŒ‰è¡Œåˆ‡åˆ†ï¼Œä¸ä½¿ç”¨é€šç”¨ TextSplitter
            chunk_size = 20
            for i in range(0, len(df), chunk_size):
                chunk_df = df.iloc[i:i + chunk_size]
                md_table = chunk_df.to_markdown(index=False)

                chunks.append(ParsedChunk(
                    content=f"[å·¥ä½œè¡¨: {sheet_name}]\n{md_table}",
                    page_number=i // chunk_size + 1,
                    content_type="table",
                ))
        return chunks

    def _parse_csv(self, file_path: str) -> List[ParsedChunk]:
        """è§£æ CSV æ–‡ä»¶"""
        chunks = []
        df = pd.read_csv(file_path)
        chunk_size = 20
        for i in range(0, len(df), chunk_size):
            chunk_df = df.iloc[i:i + chunk_size]
            md_table = chunk_df.to_markdown(index=False)

            chunks.append(ParsedChunk(
                content=md_table,
                page_number=i // chunk_size + 1,
                content_type="table",
            ))
        return chunks
