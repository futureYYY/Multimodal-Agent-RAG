"""
æ–‡ä»¶è§£æä»»åŠ¡
"""

import asyncio
from datetime import datetime
from sqlmodel import Session, select

from app.tasks.celery_app import celery_app
from app.core.database import engine
from app.models import FileDocument, DocumentChunk, FileStatus, ContentType, KnowledgeBase, CustomModel
from app.services.parser import FileParser
from app.services.vector_store import VectorStoreService
from app.services.embedding import EmbeddingService
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from sqlalchemy.exc import OperationalError

# å®šä¹‰é‡è¯•ç­–ç•¥ï¼šæ•è· OperationalError (é€šå¸¸åŒ…å« database is locked)ï¼Œæœ€å¤šé‡è¯• 5 æ¬¡ï¼ŒæŒ‡æ•°é€€é¿
db_retry = retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type(OperationalError)
)

@db_retry
def safe_commit(session: Session):
    """å¸¦é‡è¯•æœºåˆ¶çš„æ•°æ®åº“æäº¤"""
    session.commit()


def run_async(coro):
    """åœ¨åŒæ­¥ç¯å¢ƒä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(coro)
    finally:
        loop.close()


@celery_app.task(bind=True, max_retries=3)
def parse_file_task(self, file_id: str, **kwargs):
    """Celery ä»»åŠ¡åŒ…è£…å™¨"""
    try:
        return process_file_parsing(file_id, **kwargs)
    except Exception as e:
        raise self.retry(exc=e, countdown=60)


@celery_app.task(bind=True, max_retries=3)
def submit_chunks_task(self, file_id: str, chunks_data: list):
    """å¤„ç†æäº¤çš„ Chunks ä»»åŠ¡"""
    try:
        return process_submitted_chunks(file_id, chunks_data)
    except Exception as e:
        raise self.retry(exc=e, countdown=60)


def process_submitted_chunks(file_id: str, chunks_data: list):
    """
    å¤„ç†å‰ç«¯æäº¤çš„ chunks (åŒ…å«æ‰‹åŠ¨ä¿®æ”¹åçš„å†…å®¹)
    """
    print(f"ğŸš€ [SubmitTask] å¼€å§‹å¤„ç†æäº¤çš„ Chunks: {file_id}, Count={len(chunks_data)}")
    with Session(engine, expire_on_commit=False) as session:
        # è·å–æ–‡ä»¶è®°å½•
        file_doc = session.get(FileDocument, file_id)
        if not file_doc:
            print(f"âŒ [SubmitTask] æ–‡ä»¶ä¸å­˜åœ¨: {file_id}")
            return {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}

        try:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            file_doc.status = FileStatus.EMBEDDING # ç›´æ¥è¿›å…¥ Embedding é˜¶æ®µï¼Œå› ä¸ºå·²ç» Parse è¿‡äº†
            file_doc.progress = 10
            file_doc.updated_at = datetime.utcnow()
            session.add(file_doc)
            safe_commit(session)
            
            # æ¸…ç†æ—§æ•°æ®
            print("ğŸ§¹ [SubmitTask] æ¸…ç†æ—§æ•°æ®...")
            existing_chunks = session.exec(
                select(DocumentChunk).where(DocumentChunk.file_id == file_id)
            ).all()
            for c in existing_chunks:
                session.delete(c)
            safe_commit(session)

            try:
                vector_service = VectorStoreService()
                vector_service.delete_by_file_id(file_doc.kb_id, file_id)
            except Exception as e:
                print(f"âš ï¸ [SubmitTask] æ¸…ç†å‘é‡åº“å¤±è´¥: {e}")

            # ä¿å­˜ Chunks
            db_chunks = []
            print("ğŸ”„ [SubmitTask] å¼€å§‹ä¿å­˜åˆ†å—...")

            for idx, chunk_data in enumerate(chunks_data):
                # chunk_data æ˜¯å­—å…¸
                try:
                    c_type = ContentType(chunk_data.get("content_type", "text"))
                except ValueError:
                    c_type = ContentType.TEXT

                db_chunk = DocumentChunk(
                    file_id=file_id,
                    content=chunk_data.get("content", ""),
                    page_number=chunk_data.get("page_number", 0),
                    content_type=c_type,
                    image_path=chunk_data.get("image_path", ""),
                    original_index=idx,
                    vlm_description=chunk_data.get("vlm_description", ""), # æ”¯æŒ VLM æè¿°
                )
                db_chunks.append(db_chunk)
                session.add(db_chunk)
            
            # æ›´æ–°è¿›åº¦ä¸º 50% å¹¶æäº¤åˆ†å—
            file_doc.progress = 50
            session.add(file_doc)
            
            safe_commit(session) # æäº¤ä»¥è·å– ID å¹¶ä¿å­˜åˆ†å—
            print(f"âœ… [SubmitTask] åˆ†å—ä¿å­˜å®Œæˆï¼Œå…± {len(db_chunks)} ä¸ª")

            # å‘é‡åŒ–å¹¶å…¥åº“
            print("ğŸ§  [SubmitTask] å¼€å§‹ç”Ÿæˆ Embedding å¹¶å…¥åº“...")
            # åˆ·æ–°å¯¹è±¡ä»¥ç¡®ä¿åœ¨åŒä¸€äº‹åŠ¡ä¸­å¯ç”¨ï¼ˆè™½ç„¶ commit ä¼š expireï¼Œä½†æˆ‘ä»¬é©¬ä¸Šè¦ç”¨å®ƒï¼‰
            session.refresh(file_doc)

            try:
                # è·å–çŸ¥è¯†åº“ä¿¡æ¯ (ä¸ºäº†è·å– embedding_model)
                kb = session.get(KnowledgeBase, file_doc.kb_id)
                if not kb:
                    raise Exception(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {file_doc.kb_id}")
                
                embedding_model_id = kb.embedding_model
                print(f"ğŸ” [ParseTask] KnowledgeBase ID: {kb.id}")
                print(f"ğŸ” [ParseTask] Retrieved embedding_model_id from KB: '{embedding_model_id}'")

                # æ£€æŸ¥æ˜¯å¦ä¸ºè‡ªå®šä¹‰æ¨¡å‹
                custom_model = session.get(CustomModel, embedding_model_id)
                
                embedding_service = None
                if custom_model:
                    print(f"ğŸ” [ParseTask] Using Custom Model: {custom_model.name} ({custom_model.model_name})")
                    embedding_service = EmbeddingService(
                        base_url=custom_model.base_url,
                        api_key=custom_model.api_key,
                        model=custom_model.model_name
                    )
                    # ä½¿ç”¨å®é™…æ¨¡å‹åç§°è¦†ç›– ID
                    embedding_model_id = custom_model.model_name
                else:
                    print(f"ğŸ” [ParseTask] Using System/Default Model: {embedding_model_id}")
                    embedding_service = EmbeddingService()

                vector_service = VectorStoreService()

                documents_for_chroma = []
                contents_to_embed = []

                for chunk in db_chunks:
                    text_content = chunk.content
                    
                    metadata = {
                        "file_id": file_id,
                        "file_name": file_doc.name,
                        "chunk_index": chunk.original_index,
                        "page_number": chunk.page_number or 0,
                        "content_type": chunk.content_type.value,
                        "image_path": chunk.image_path or "",
                        "location_info": f"Page {chunk.page_number or 1} | Chunk #{chunk.original_index + 1}"
                    }
                    
                    documents_for_chroma.append({
                        "id": chunk.id,
                        "content": text_content,
                        "metadata": metadata
                    })
                    contents_to_embed.append(text_content)

                print(f"   -> æ­£åœ¨ä¸º {len(contents_to_embed)} ä¸ªå—ç”Ÿæˆå‘é‡...")
                embeddings = run_async(embedding_service.embed_documents(contents_to_embed, model_id=embedding_model_id))
                
                print(f"   -> æ­£åœ¨å†™å…¥ ChromaDB...")
                # ç”±äº embedding è¿‡ç¨‹æ˜¯å¼‚æ­¥çš„ï¼Œsession å¯èƒ½ä¼šå› ä¸º expire_on_commit=True è€Œå¤±æ•ˆ
                # è™½ç„¶æˆ‘ä»¬ refresh è¿‡äº†ï¼Œä½†ä¸ºäº†ä¿é™©ï¼Œè¿™é‡Œå†æ¬¡ refresh æˆ–è€… merge
                # ä¸è¿‡ file_doc åœ¨è¿™é‡Œå¹¶ä¸éœ€è¦ updateï¼Œæˆ‘ä»¬åªæ˜¯ç”¨ kb_id
                
                vector_service.add_documents(
                    kb_id=file_doc.kb_id,
                    documents=documents_for_chroma,
                    embeddings=embeddings
                )
                print("âœ… [SubmitTask] å‘é‡å…¥åº“å®Œæˆï¼")

            except Exception as embed_err:
                print(f"âŒ [SubmitTask] å‘é‡åŒ–å¤±è´¥: {embed_err}")
                raise embed_err

            # å®Œæˆ
            # è¿™é‡Œå¿…é¡»é‡æ–°è·å– file_docï¼Œå› ä¸ºç»å†äº†è€—æ—¶çš„ embedding è¿‡ç¨‹ï¼Œ
            # ä¸”ä¹‹å‰çš„ commit å¯èƒ½å¯¼è‡´å¯¹è±¡è¿‡æœŸæˆ– detached
            file_doc = session.get(FileDocument, file_id)
            if file_doc:
                file_doc.status = FileStatus.PARSED
                file_doc.progress = 100
                file_doc.updated_at = datetime.utcnow()
                session.add(file_doc)
                safe_commit(session)

                # æ›´æ–°çŸ¥è¯†åº“çš„ chunk_count
                try:
                    kb = session.get(KnowledgeBase, file_doc.kb_id)
                    if kb:
                        # ç»Ÿè®¡è¯¥çŸ¥è¯†åº“ä¸‹æ‰€æœ‰æ–‡ä»¶çš„ chunks æ€»æ•°
                        from sqlalchemy import func
                        total_chunks = session.exec(
                            select(func.count(DocumentChunk.id))
                            .join(FileDocument)
                            .where(FileDocument.kb_id == kb.id)
                        ).one()
                        
                        kb.chunk_count = total_chunks
                        kb.updated_at = datetime.utcnow()
                        session.add(kb)
                        safe_commit(session)
                        print(f"âœ… [SubmitTask] æ›´æ–°çŸ¥è¯†åº“ {kb.id} çš„ chunk_count ä¸º {total_chunks}")
                except Exception as kb_err:
                    print(f"âš ï¸ [SubmitTask] æ›´æ–°çŸ¥è¯†åº“ chunk_count å¤±è´¥: {kb_err}")

            print("ğŸ‰ [SubmitTask] ä»»åŠ¡å®Œæˆã€‚")

        except Exception as e:
            print(f"âŒ [SubmitTask] å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            file_doc = session.get(FileDocument, file_id)
            if file_doc:
                file_doc.status = FileStatus.FAILED
                file_doc.error_message = str(e)
                session.add(file_doc)
                safe_commit(session)
            raise e


def process_file_parsing(
    file_id: str,
    chunk_mode: str = "auto",
    chunk_size: int = 500,
    chunk_overlap: int = 50,
    separator: str = "\n\n",
    auto_vectorize: bool = False
):
    """
    è§£ææ–‡ä»¶æ ¸å¿ƒé€»è¾‘ï¼ˆè§£è€¦ Celeryï¼‰
    """
    print(f"ğŸš€ [ParseTask] å¼€å§‹å¤„ç†æ–‡ä»¶è§£æ: {file_id}, Mode={chunk_mode}")
    with Session(engine) as session:
        # è·å–æ–‡ä»¶è®°å½•
        file_doc = session.get(FileDocument, file_id)
        if not file_doc:
            print(f"âŒ [ParseTask] æ–‡ä»¶ä¸å­˜åœ¨: {file_id}")
            return {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}

        try:
            print(f"ğŸ“„ [ParseTask] æ–‡ä»¶ä¿¡æ¯: {file_doc.name}, è·¯å¾„: {file_doc.local_path}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºè§£æä¸­
            file_doc.status = FileStatus.PARSING
            file_doc.progress = 0
            file_doc.updated_at = datetime.utcnow()
            session.add(file_doc)
            safe_commit(session)
            
            # æ¸…ç†æ—§çš„ Chunks (æ”¯æŒé‡è§£æ)
            print("ğŸ§¹ [ParseTask] æ¸…ç†æ—§æ•°æ®...")
            existing_chunks = session.exec(
                select(DocumentChunk).where(DocumentChunk.file_id == file_id)
            ).all()
            for c in existing_chunks:
                session.delete(c)
            safe_commit(session)

            # æ¸…ç†å‘é‡åº“ä¸­çš„æ—§æ•°æ®
            try:
                vector_service = VectorStoreService()
                vector_service.delete_by_file_id(file_doc.kb_id, file_id)
                print(f"ğŸ§¹ [ParseTask] å·²ä»å‘é‡åº“æ¸…ç†æ–‡ä»¶ {file_id} çš„æ•°æ®")
            except Exception as e:
                print(f"âš ï¸ [ParseTask] æ¸…ç†å‘é‡åº“å¤±è´¥ (å¯èƒ½ä¹‹å‰æœªå…¥åº“): {e}")

            # åˆå§‹åŒ–è§£æå™¨
            parser = FileParser(kb_id=file_doc.kb_id)

            # è§£ææ–‡ä»¶
            file_doc.progress = 10
            session.add(file_doc)
            safe_commit(session)
            print("âœ… [ParseTask] å¼€å§‹è°ƒç”¨ parser.parse()...")

            # åŒæ­¥è°ƒç”¨è§£æ
            try:
                parsed_chunks = parser.parse(
                    file_path=file_doc.local_path,
                    chunk_mode=chunk_mode,
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    separator=separator
                )
                print(f"âœ… [ParseTask] è§£æå®Œæˆï¼Œè·å¾— {len(parsed_chunks)} ä¸ªå—")
            except Exception as parse_err:
                print(f"âŒ [ParseTask] parser.parse() å†…éƒ¨æŠ›å‡ºå¼‚å¸¸: {parse_err}")
                import traceback
                traceback.print_exc()
                raise parse_err

            # ä¿å­˜ Chunks
            db_chunks = []
            print("ğŸ”„ [ParseTask] å¼€å§‹ä¿å­˜åˆ†å—...")

            for idx, chunk in enumerate(parsed_chunks):
                # æ˜ å°„ ContentType
                try:
                    c_type = ContentType(chunk.content_type)
                except ValueError:
                    c_type = ContentType.TEXT

                db_chunk = DocumentChunk(
                    file_id=file_id,
                    content=chunk.content,
                    page_number=chunk.page_number,
                    content_type=c_type,
                    image_path=chunk.image_path,
                    original_index=idx,
                )
                db_chunks.append(db_chunk)
                session.add(db_chunk)

            print(f"âœ… [ParseTask] åˆ†å—å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(db_chunks)} ä¸ªæ•°æ®åº“è®°å½•å—")

            # æ›´æ–°æ–‡ä»¶çŠ¶æ€ä¸ºè§£æå®Œæˆ (PENDING_CONFIRM)
            file_doc.status = FileStatus.PENDING_CONFIRM
            file_doc.progress = 50
            file_doc.updated_at = datetime.utcnow()
            session.add(file_doc)
            
            # æš‚æ—¶ä¸æ›´æ–°çŸ¥è¯†åº“åˆ†å—è®¡æ•°ï¼Œå› ä¸ºè¿˜æœªå…¥åº“
            # ...

            safe_commit(session)
            print("ğŸ‰ [ParseTask] è§£æå®Œæˆï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤å…¥åº“ã€‚")
            
            # å‡†å¤‡è‡ªåŠ¨å‘é‡åŒ–çš„æ•°æ®
            auto_vectorize_data = None
            if auto_vectorize:
                print(f"ğŸš€ [ParseTask] è‡ªåŠ¨è§¦å‘å‘é‡åŒ–å…¥åº“: {file_id}")
                auto_vectorize_data = [
                    {
                        "content": c.content,
                        "page_number": c.page_number,
                        "content_type": c.content_type.value,
                        "image_path": c.image_path,
                        "vlm_description": c.vlm_description if hasattr(c, "vlm_description") else None,
                    }
                    for c in db_chunks
                ]

            # å³ä½¿åœ¨ Session ä¸Šä¸‹æ–‡ä¸­ï¼Œåªè¦ commit äº†ï¼Œè¿æ¥é”å°±é‡Šæ”¾äº†ï¼ˆå¯¹äº WALï¼‰
            # ä½¿ç”¨ QueuePool æ—¶ï¼Œprocess_submitted_chunks ä¼šè·å–ä¸€ä¸ªæ–°çš„è¿æ¥
            if auto_vectorize_data:
                process_submitted_chunks(file_id, auto_vectorize_data)

            return {
                "file_id": file_id,
                "chunk_count": len(db_chunks) if 'db_chunks' in locals() else 0,
                "status": "pending_confirm",
            }

        except Exception as e:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            print(f"âŒ [ParseTask] å…¨å±€å¼‚å¸¸æ•è·: {e}")
            import traceback
            traceback.print_exc()
            
            # é‡æ–°è·å– session ä¸­çš„å¯¹è±¡ä»¥é˜²è¿‡æœŸ
            file_doc = session.get(FileDocument, file_id)
            if file_doc:
                file_doc.status = FileStatus.FAILED
                file_doc.error_message = str(e)
                file_doc.updated_at = datetime.utcnow()
                session.add(file_doc)
                safe_commit(session)
            
            raise e
